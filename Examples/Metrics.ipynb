{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import notebooks.src.visualizingrestatements.visualizingrestatements as vs\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "from botocore.config import Config\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtk\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.dates as mdates\n",
    "from math import ceil\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector = npt.ArrayLike\n",
    "Array = npt.ArrayLike  # column number and row number represent the same day, so diagonal should be ignored\n",
    "DictDfs = dict\n",
    "DictArrays = dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(value: float, truth: float)-> float:\n",
    "    if truth != 0 and truth is not np.nan:\n",
    "        return (truth - value)/truth\n",
    "\n",
    "    if truth == 0:\n",
    "        if value == 0:\n",
    "            return 0\n",
    "        if value != 0:\n",
    "            return np.sign(value) * np.inf\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def offset_accuracy(X: npt.ArrayLike, K: int = 45) -> Vector:\n",
    "    vals = []\n",
    "\n",
    "    for i in range(X.shape[0] - K):\n",
    "        t_hat = X[i, i + K]\n",
    "        first_reported = X[i, i+1]  ## column number and row number represent the same day\n",
    "        \n",
    "        if t_hat == 0 or np.isnan(first_reported) or np.isnan(t_hat):\n",
    "            vals.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            vals.append( accuracy(first_reported, t_hat) )\n",
    "    \n",
    "    return vals\n",
    "\n",
    "\n",
    "def naive_accuracy(X: npt.ArrayLike, m: int) -> Vector:\n",
    "\n",
    "    vals = []\n",
    "\n",
    "    for i in range(X.shape[0] -1):\n",
    "        t_hat = X[i, m]\n",
    "        first_reported = X[i, i+1]\n",
    "\n",
    "        if t_hat == 0 or np.isnan( first_reported) or np.isnan(t_hat):\n",
    "            vals.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            vals.append( accuracy( first_reported, t_hat ))\n",
    "\n",
    "    return vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_with_average( measurements: Vector, history: int = 0) -> Vector:\n",
    "    \"\"\"Return moving or cumulative (history=0) average of not-nan values\"\"\"\n",
    "    \n",
    "    m_exists = [ 1 if not exists else 0 for exists in np.isnan( measurements)]    \n",
    "    m_values = [ val if exists else 0 for exists, val in zip( m_exists, measurements)]\n",
    "\n",
    "    if history == 0:\n",
    "        denom = [ sum( m_exists[ 0 : i + 1]) for i in range( len( m_exists) )]\n",
    "        numer = [ sum( m_values[ 0 : i + 1]) for i in range( len( m_values) )]\n",
    "\n",
    "    else:\n",
    "        denom = [ sum( m_exists[ max(0, i - history) : i + 1]) for i in range( len( m_exists) )]\n",
    "        numer = [ sum( m_values[ max(0, i - history) : i + 1]) for i in range( len( m_values) )]\n",
    "\n",
    "    estimates = [n/d if d != 0 else np.nan for n, d in zip(numer, denom)]\n",
    "\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " np.cumsum( [1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def performance_estimates( measurements: Vector, offset: int, **kwargs) -> Vector:\n",
    "\n",
    "    diffs = [np.nan] * offset\n",
    "    ests = estimate_with_average( measurements, **kwargs) # rolling average\n",
    "\n",
    "    for i in range( len( ests) - offset):\n",
    "        diffs.append( measurements[ i + offset] - ests[ i])  # we can only use up to (today - offset) to estimate today's accuracy\n",
    "    \n",
    "    return diffs\n",
    "\n",
    "## return the actual not the difference \n",
    "def accuracy_estimates_all( measurements_df: list, releases: list, **kwargs) -> pd.DataFrame:\n",
    "    \n",
    "    dct = {}\n",
    "\n",
    "    for offset, accuracies in measurements_df.iteritems():\n",
    "        waiting = [np.nan] * int(offset)\n",
    "        ests =  estimate_with_average( accuracies, **kwargs) # rolling average\n",
    "\n",
    "        dct[offset] = waiting + ests[: len(accuracies) -int(offset)]\n",
    "\n",
    "    return pd.DataFrame(dct, index= releases)\n",
    "\n",
    "\n",
    "\n",
    "def diff_matrix( measurements: list, offsets: int, columns: list, **kwargs) -> pd.DataFrame:\n",
    "    \n",
    "    perf_ests = []\n",
    "\n",
    "    for accuracy_dm, offset in zip( measurements, offsets):\n",
    "        perf_ests.append( performance_estimates(accuracy_dm, offset, **kwargs))\n",
    "\n",
    "\n",
    "    return pd.DataFrame(perf_ests, index=[f'{each}' for each in offsets], columns=columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness_and_timeliness( release: npt.ArrayLike, release_num: int)-> float:\n",
    "\n",
    "    j = release_num - 1\n",
    "    empty = True\n",
    "    \n",
    "    while empty:\n",
    "\n",
    "        if release[j] is not np.nan:\n",
    "            empty= False\n",
    "        \n",
    "        else:\n",
    "            j -= 1\n",
    "\n",
    "            if j < 0:\n",
    "                return 0\n",
    "\n",
    "    return (1 - (( (release_num - 1) - j)/ release_num)**2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeliness_and_accuracy(observation: npt.ArrayLike, obsv_num: int, offset: int, epsilon: float) -> int:\n",
    "    \n",
    "    obsvs = observation[obsv_num + 1 : obsv_num + offset ]\n",
    "    truth_estimate = obsvs[-1] # last reported value\n",
    "    \n",
    "    if truth_estimate is np.nan:\n",
    "        return np.nan\n",
    "\n",
    "    j = offset\n",
    "    equal = True\n",
    "\n",
    "    while equal:\n",
    "        equal = ( abs( accuracy( truth_estimate, observation[ obsv_num + j]) ) < epsilon )\n",
    "\n",
    "        if equal:\n",
    "            j -= 1\n",
    "\n",
    "            if j == 1:\n",
    "                return 1\n",
    "\n",
    "    return j\n",
    "\n",
    "        \n",
    "def timeliness_and_accuracy_all(X: npt.ArrayLike, offset: int, epsilon: float) -> Vector:\n",
    "\n",
    "    taa = []\n",
    "    for i in range(X.shape[0] - offset):\n",
    "        taa.append(timeliness_and_accuracy(X[i, :], i, offset, epsilon))\n",
    "\n",
    "    return taa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness(X: Array, i: int) -> float:\n",
    "    return X[i, i+1] is not np.nan\n",
    "\n",
    "def completeness_group(df_dict : dict, i: int) -> float:\n",
    "\n",
    "    m = len(df_dict .keys())\n",
    "    indicators = []\n",
    "\n",
    "    for key, df in df_dict .items():\n",
    "        indicators.append( completeness(df, i) )\n",
    "\n",
    "    return sum(indicators) / m\n",
    "\n",
    "def check_dfs_dict(df_dict: dict ) -> tuple:\n",
    "\n",
    "    key_iter = iter(df_dict )\n",
    "    key = next(key_iter)\n",
    "    columns, inds = df_dict [key].columns, df_dict [key].index\n",
    "    keys = [key]\n",
    "\n",
    "    while (key := next(key_iter, None)) is not None:\n",
    "        col, ind = df_dict [key].columns, df_dict [key].index\n",
    "        \n",
    "        if (col != columns).any():\n",
    "            return False, key, keys, \"columns\"\n",
    "\n",
    "        if (ind != inds).any():\n",
    "            return False, key, keys, \"indices\"\n",
    "    \n",
    "    return True, (inds, columns)\n",
    "\n",
    "def completeness_group_all(arrays_dict: dict , df_dict: dict ) -> pd.DataFrame:\n",
    "    # returns completeness of RELEASES, ie release i --> does X[i, i+1] exist\n",
    "    \n",
    "    aligned, tup = check_dfs_dict(df_dict )\n",
    "    if not aligned:\n",
    "        return \"MISTMATCHED\"\n",
    "\n",
    "    inds, columns = tup\n",
    "    vals = []\n",
    "\n",
    "    # completeness for i, i+1\n",
    "    for i in range(len(columns)-1):\n",
    "        # i is obsv, i+1 is release\n",
    "        vals.append( completeness_group(arrays_dict , i) )\n",
    "    \n",
    "    return pd.DataFrame([vals], columns=columns[1:], index=[\"Completeness\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def major_restatement(i: int, release_i: npt.ArrayLike, release_ip1: npt.ArrayLike, beta: float=.2, alpha: float = 0) -> dict:\n",
    "\n",
    "    revisions = sum( abs(release_i[: (i-1)] - release_ip1[: i-1]) > alpha)\n",
    "    # a release i+1 can change observations 0 to i-1 (last value reported in release i)   (which is i values)\n",
    "    if i == 0:\n",
    "         return { \"major_restatement\": 0>beta,  \"percent\": 0,  \"number\": 0}   \n",
    "    return { \"major_restatement\": (revisions/( i )) > beta,  \"percent\": (revisions/( i )),  \"number\": revisions}\n",
    "\n",
    "def major_restatements_all(X: npt.ArrayLike, release_names: npt.ArrayLike, **kwargs) -> dict:\n",
    "\n",
    "    mrs = {}\n",
    "    for i in range(1, X.shape[1] -1): # no release 0\n",
    "        mrs[release_names[i+1]] = major_restatement(i , X[:, i], X[:, i +1 ], **kwargs)\n",
    "\n",
    "    return mrs\n",
    "\n",
    "def major_restatements_group_all(arrays_dict: dict , df_dict:dict ,  **kwargs) -> pd.DataFrame:\n",
    "    \n",
    "    aligned, tup = check_dfs_dict(df_dict )\n",
    "    if not aligned:\n",
    "        return \"MISTMATCHED\"\n",
    "\n",
    "    inds, columns = tup\n",
    "    vals = []\n",
    "\n",
    "    for key, X in arrays_dict .items():\n",
    "        \n",
    "        dct = major_restatements_all(X, columns,  **kwargs)\n",
    "        for release, sub_dict  in dct.items():\n",
    "            sub_dict[\"region\"] = key\n",
    "            sub_dict[\"release\"] = release\n",
    "\n",
    "            vals.append(sub_dict )\n",
    "    \n",
    "    return pd.DataFrame(vals)\n",
    "\n",
    "def consistency_mr_concurrence(arrays_dict:dict , df_dict:dict , **kwargs) -> dict:\n",
    "\n",
    "    ## use np array iterate thorugh dates\n",
    "\n",
    "    aligned, tup = check_dfs_dict(df_dict )\n",
    "    if not aligned:\n",
    "        return \"MISTMATCHED\"\n",
    "    \n",
    "    inds, columns = tup\n",
    "    vals = {}\n",
    "    m = len(arrays_dict .keys())\n",
    "\n",
    "    for i in range(len(columns) - 1):\n",
    "        \n",
    "        mrs = {}\n",
    "        for region, X in arrays_dict .items():\n",
    "\n",
    "            dct = major_restatement(i , X[:, i], X[:, i +1], **kwargs)\n",
    "            mrs[region] = dct[\"major_restatement\"]\n",
    "\n",
    "        total = sum(mrs.values())\n",
    "       \n",
    "        dct = {\n",
    "            region: 1 - abs(val - (1/ (m-1))*(total-val) )\n",
    "            for region, val in mrs.items()\n",
    "        }\n",
    "        dct['group'] = 2 * abs((1/m) * total  - .5)\n",
    "\n",
    "        vals[columns[i+1]] =  dct\n",
    "\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validity(release: npt.ArrayLike, i: int) -> bool:\n",
    "\n",
    "    return release[i] >= release[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validity_all(X: npt.ArrayLike, release_names: npt.ArrayLike) -> dict:\n",
    "\n",
    "    vals = {}\n",
    "    for i in range(1, X.shape[1]-1):\n",
    "        rel = release_names[i]\n",
    "        vals[rel] = validity(X[:,i], i-1)\n",
    "\n",
    "    return vals\n",
    "\n",
    "def validity_group_all(arrays_dict: dict , df_dict:dict ,  **kwargs) -> pd.DataFrame:\n",
    "\n",
    "    aligned, tup = check_dfs_dict(df_dict )\n",
    "    if not aligned:\n",
    "        return \"MISTMATCHED\"\n",
    "\n",
    "    inds, columns = tup\n",
    "    vals = {}\n",
    "\n",
    "    for key, X in arrays_dict.items():\n",
    "        \n",
    "        vals[key] = validity_all(X, df_dict[key].columns)\n",
    "\n",
    "    return pd.DataFrame(vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT ADD BELOW TO SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../latex/code/notebooks/Metrics.py'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jupyckage.jupyckage as jp \n",
    "import shutil\n",
    "\n",
    "to_latex_folder = \"../latex/code/notebooks/\"\n",
    "\n",
    "notebook_name = \"Metrics\"\n",
    "\n",
    "jp.notebook_to_package(f'{notebook_name}.ipynb')\n",
    "\n",
    "shutil.copy( f'notebooks/src/{notebook_name}/{notebook_name}.py', f'{to_latex_folder}{notebook_name}.py')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kerstin/SuperUse/usability/usability/Metrics.py'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_repo_path = \"/Users/kerstin/SuperUse/usability/usability/\"\n",
    "shutil.copy( f'notebooks/src/{notebook_name}/{notebook_name}.py', f'{github_repo_path}{notebook_name}.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "696614e554172991e4cdcae24582ddba472d4b725082833a9c468eca202a50d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
